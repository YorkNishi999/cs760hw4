\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{paralist}
\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}


\urlstyle{rm}

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\theoremstyle{definition}
\newtheorem{definition}{Definition}[]
\newtheorem{conjecture}{Conjecture}[]
\newtheorem{example}{Example}[]
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\br}[1]{\{#1\}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\qedsymbol}{$\blacksquare$}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\xv}{\vc{x}}
\newcommand{\Sigmav}{\vc{\Sigma}}
\newcommand{\alphav}{\vc{\alpha}}
\newcommand{\muv}{\vc{\mu}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\def\x{\mathbf x}
\def\y{\mathbf y}
\def\w{\mathbf w}
\def\v{\mathbf v}
\def\E{\mathbb E}
\def\V{\mathbb V}

% TO SHOW SOLUTIONS, include following (else comment out):
\newenvironment{soln}{
	\leavevmode\color{blue}\ignorespaces
}{}


\hypersetup{
	%    colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

\geometry{
	top=1in,            % <-- you want to adjust this
	inner=1in,
	outer=1in,
	bottom=1in,
	headheight=3em,       % <-- and this
	headsep=2em,          % <-- and this
	footskip=3em,
}


\pagestyle{fancyplain}
\lhead{\fancyplain{}{Homework 4}}
\rhead{\fancyplain{}{CS 760 Machine Learning, Fall 2021}}
\cfoot{\thepage}

\title{\textsc{Homework 4}} % Title

%%% NOTE:  Replace 'NAME HERE' etc., and delete any "\red{}" wrappers (so it won't show up as red)

\author{
	Yohei Nishimura \\
	ynishimura \\
} 

\date{}

\begin{document}
	
	\maketitle 
		
	\section*{Solution 1}
      		\subsection*{Solution 1.1: Strategy 1}
     		 \begin{soln}
		 If we $\hat{x} \in argmax_x \theta_x$, and if let j be the x at which the largest probability is taken,  then $E[1[\hat{x} = j]] = \theta_j$. Therefore, $E[1[\hat{x} \neq j]] = 1 - \theta_j$. By changing this notation j to x, we get the following equation.
 		     \[
 		     E[1[\hat{x} \neq x]] = 1 - \theta_x
 		     \]
		  \end{soln}
      
   		   \subsection*{Solution 1.2: Strategy 2}
		   \begin{soln}
  		    Suppose $\hat{x}$ is generated from a multinomial distribution. $\hat{x} = x$ because the probability of $x$ and $\hat{x}$ occurring is $\theta_x$, which is $\theta^2_x$ since the two events are independent. Therefore, we get below:
  		    \[
 		     E[1[\hat{x} \neq x]] = 1 - \theta^2_x
  		    \]
		 \end{soln}
      
      	\section*{Solution 2}
	\begin{soln}
		      The probability that $x=i$ occurs can be expressed as $\theta_i$, and the probability that $\hat{x}=j$ occurs can be expressed as $\theta_j$. Therefore, we can write $c_{i,j} = \theta_i \theta_i$.

Under the knowledge of $\theta$, to minimize the expected value of $c_{i,j}$, it is better to set $\hat{x}$ to be the index representing the lowest probability among $\theta$ vector. Because $\theta_i$ is given and what  we can not control, $\theta_i$ is treated as a constant. As a result, to minimize $c_{i,j} = \theta_i \theta_i$, it is a necessary and sufficient condition to minimize $\theta_j$.	
	\end{soln}
	
	\section*{Solution 3}
	\subsection*{Solution 3.1}
	\begin{soln}
	Let the mean of the distribution of $Y_t$ be $\mu$ and the variance be $\sigma^2$. In the setting of this question, since we know these two variables, the optimal strategy is to set $x_t = \mu$ at any time. It is because  $E[y_t] = \mu$, meaning that the value of $y_t$ that is most likely to appear is $\mu$.

	At this time, the expected payment for the $T$th time is as follows:
		\begin{equation*}\label{xx}
			\begin{split}
		E[T (x_t - y_t)^2] &= T (\mu^2 - 2 \mu E[y_t] + E[y_t^2)]) = T (\mu^2 - 2 \mu^2 + \sigma^2 + \mu^2) \\
		&= T \sigma^2
			\end{split}
		\end{equation*}	
During the development of the above equation, I used the following:
		\begin{equation*}\label{xx}
			\begin{split}
			Var(y_t) = \sigma^2 &= E[y_t^2] - (E[y_t])^2 = E[y_t^2] - \mu^2 \\
			\therefore E[y_t^2] &=  \sigma^2 + \mu^2
			\end{split}
		\end{equation*}	
	
	\end{soln}

	\subsection*{Solution 3.2}
	\begin{soln}
		Based on previous problem, we can set the "payment" function below:
		\begin{equation}\label{xx}
			\begin{split}
			argmin_{x_t} Payment = T ( E[x_t]^2 - 2 E[x_t] E[y_t] + E[y_t^2)])
			\end{split}
		\end{equation}	
		At the end of the $m$th enforcement, calculate the following with the realized values.
		\begin{equation*}\label{xx}
			\begin{split}
			E[x_t] &= \frac{1}{m} \sum_{i=1}^{m} x_i = \bar{x} \\
			E[x_t]^2 &= \frac{1}{m} \sum_{i=1}^{m} (x_i - \bar{x} )^2   + \bar{x}^2 = Var(x) +\bar{x}^2 \\
			E[y_t] &= \frac{1}{m} \sum_{i=1}^{m} y_i = \bar{y} \\
			E[y_t]^2 &= \frac{1}{m} \sum_{i=1}^{m} (y_i - \bar{y} )^2   + \bar{y}^2 = Var(y) +\bar{y}^2 \\
			\end{split}
		\end{equation*}	
		Then, substitute the above four equations into equation (1).
		\begin{equation}\label{xx}
			\begin{split}
			argmin_{x_t} Payment &= T ( \frac{1}{m} \sum_{i=1}^{m} (x_i - \bar{x} )^2   + \bar{x}^2 +  \frac{1}{m} \sum_{i=1}^{m} (y_i - \bar{y} )^2   + \bar{y}^2 - 2 \bar{x} \bar{y} )  \\
			&= T ( Var(x) + Var(y) + (\bar{x} - \bar{y} )^2 ) \\
			\end{split}
		\end{equation}	
		Calculate equation (2) above for the $m+1$th time as well, and if equation (2), or the benchmark, is lower than the $m$th time, we can conclude that the strategy we are currently using is correct. In other words, equation (2) is the benchmark.
	\end{soln}
	
	
	\section*{Solution 4}
	\subsection*{Solution 4.1}	
	\begin{soln}
	From the setting of this question, $N = 30$, $K_L = 3$, and $\alpha = 0.5$. From the setup of this question, $N = 30$, $K_L = 3$, and $\alpha = 0.5$. Using these, calculate the prior probabilities.
		\begin{equation*}\label{xx}
			\begin{split}
				\hat{p}_{\alpha}(y = e) &= \frac{\sum_{i=1}^{N} 1[y^{(i)} = e] + \alpha }{N + K_L \alpha} \\ 
				&= \frac{10 + 0.5}{30 + 3*0.5} = \frac{3}{10} = 0.33 \\
			\end{split}
		\end{equation*}	
		Similarly, the two prior probabilities of ta are calculated and the answers are as follows.
		\begin{equation*}\label{xx}
			\begin{split}
				\hat{p}_{\alpha}(y = e) &= 	\hat{p}_{\alpha}(y = j) = \hat{p}_{\alpha}(y = s) =  \frac{3}{10} = 0.33 \\
			\end{split}
		\end{equation*}	
		
	\end{soln}
	
	\clearpage
	\subsection*{Solution 4.2}		
	\begin{soln}
	From the given conditions, calculate the following using $p(y = e) = \frac{1}{3}$.
		\begin{equation*}\label{xx}
			\begin{split}
				\theta_{i,e} = \hat{p}(c_i | y = e) = \frac{\hat{p}(c_i \cap y=e )}{\hat{p}(y=e)} = \frac{10 c_i}{3 M_e} 
			\end{split}
		\end{equation*}		
	Here, $M_e$ represents the total number of characters in the data labeled $y = e$. Then, compute the $\theta$ for each $i$. $\theta_e$ is below:
\begin{table}[ht]
 \caption{$\theta_e$}
 \centering
  \begin{tabular}{clll}
   \hline
   character & probability \\
   \hline \hline
a & 0.06 \\
b & 0.01 \\
c & 0.02 \\
d & 0.02 \\
e & 0.11 \\
f & 0.02 \\
g & 0.02 \\
h & 0.05 \\
i & 0.05 \\
j & 0.0 \\
k & 0.0 \\
l & 0.03 \\
m & 0.02 \\
n & 0.06 \\
o & 0.07 \\
p & 0.02 \\
q & 0.0 \\
r & 0.05 \\
s & 0.07 \\
t & 0.08 \\
u & 0.03 \\
v & 0.01 \\
w & 0.02 \\
x & 0.0 \\
y & 0.01 \\
z & 0.0 \\
(space)  & 0.18 \\
   \hline
  \end{tabular}
\end{table}
	\end{soln}	

	\clearpage
	\subsection*{Solution 4.3}	
	\begin{soln}

\begin{table}[h]
  \begin{minipage}[t]{.45\textwidth}
    \begin{center}
        \caption{$\theta_j$}
      \begin{tabular}{clll}
   \hline
   character & probability \\
   \hline \hline

a & 0.13 \\
b & 0.01 \\
c & 0.01 \\
d & 0.02 \\
e & 0.06 \\
f & 0.0 \\
g & 0.01 \\
h & 0.03 \\
i & 0.1 \\
j & 0.0 \\
k & 0.06 \\
l & 0.0 \\
m & 0.04 \\
n & 0.06 \\
o & 0.09 \\
p & 0.0 \\
q & 0.0 \\
r & 0.04 \\
s & 0.04 \\
t & 0.06 \\
u & 0.07 \\
v & 0.0 \\
w & 0.02 \\
x & 0.0 \\
y & 0.01 \\
z & 0.01 \\
(space)  & 0.12 \\
   \hline
      \end{tabular}
    \end{center}

  \end{minipage}
  %
  \hfill
  %
  \begin{minipage}[t]{.45\textwidth}
    \begin{center}
       \caption{$\theta_s$}
      \begin{tabular}{clll}
   \hline
   character & probability \\
   \hline \hline
a & 0.1 \\
b & 0.01 \\
c & 0.04 \\
d & 0.04 \\
e & 0.11 \\
f & 0.01 \\
g & 0.01 \\
h & 0.0 \\
i & 0.05 \\
j & 0.01 \\
k & 0.0 \\
l & 0.05 \\
m & 0.03 \\
n & 0.05 \\
o & 0.07 \\
p & 0.02 \\
q & 0.01 \\
r & 0.06 \\
s & 0.07 \\
t & 0.04 \\
u & 0.03 \\
v & 0.01 \\
w & 0.0 \\
x & 0.0 \\
y & 0.01 \\
z & 0.0 \\
(space)  & 0.17 \\
   \hline
      \end{tabular}
    \end{center}
  \end{minipage}
\end{table}
	
	\end{soln}
	\clearpage
	\subsection*{Solution 4.4}	
	\begin{soln}
	\begin{table}[ht]
 \caption{x of e10.txt}
 \centering
  \begin{tabular}{clll}
   \hline
   character & probability \\
   \hline \hline
a & 164 \\
b & 32 \\
c & 53 \\
d & 57 \\
e & 311 \\
f & 55 \\
g & 51 \\
h & 140 \\
i & 140 \\
j & 3 \\
k & 6 \\
l & 85 \\
m & 64 \\
n & 139 \\
o & 182 \\
p & 53 \\
q & 3 \\
r & 141 \\
s & 186 \\
t & 225 \\
u & 65 \\
v & 31 \\
w & 47 \\
x & 4 \\
y & 38 \\
z & 2 \\
(space)  & 498 \\
   \hline
  \end{tabular}
\end{table}
	\end{soln}

	\clearpage
	\subsection*{Solution 4.5}	
	\begin{soln}
	Taking the logarithm of the equation, we get $\log \hat{p}(x|y) = \sum_{i=1}^{d} x_i \theta_{i,y} $. Based on previous problems, $\log \hat{p}(x|y=e) = -7840.77$, $\log \hat{p}(x|y=j) = -8727.17$, and $\log \hat{p}(x|y=s) = -8478.20. $
	\end{soln}
	
	\subsection*{Solution 4.6}	
	\begin{soln}
	Taking the logarithm of the Bayes law, we get $\log \hat{p}(y|x) = \log \hat{p}(x|y) + \log \hat{p}(y)$. Then, based on previous problems, $\log \hat{p}(y=e|x) = -7841.87$, $\log \hat{p}(y=j|x) = -8728.27$, and $\log \hat{p}(y=s|x) = -8479.30. $
	\end{soln}
	
	\subsection*{Solution 4.7}
	\begin{soln}
	The answer table is below.
	\begin{table}[htbp]
  	\centering
	 \caption{Classification among English, Spanish and Japanese}
  	\begin{tabular}{clll}
  	    & English & Spanish & Japanese \\ 
	   \hline
  	  English & 10 & 0 & 0 \\
  	  Spanish & 0 & 10 & 0 \\ 
	  Japanese & 0 & 0 & 10 \\
 	 \end{tabular}
	\end{table}
	
	\end{soln}
	
	\subsection*{Solution 4.8}
	\begin{soln}	
	Shuffling the order of the letters does not affect the results of the estimation using this Naive Bayes classifier’s prediction. This is because the order information has been omitted when creating the bag-of-words for each training and validation data, i.e. mathematically, when calculating the likelihood.
	\end{soln}
	
	
	\section*{Solution 5}
	\subsection*{Solution 5.1}	
	\begin{soln}
	Set $z$ as $z = W_2 \sigma(W_1 x)$ and $\delta_{i,j} = 1 ($when $i=j)$, $0$ otherwise.
		\begin{equation*}\label{xx}
			\begin{split}
\frac{\partial L}{\partial x} &= \frac{\partial L}{\partial \hat{y}} \frac{\partial g(z)}{\partial z} \frac{\partial z}{\partial \sigma(W_1 x)} \frac{\partial \sigma(W_1 x)}{\partial (W_1 x)} \frac{\partial W_1 x}{\partial x} \\
				&= - \sum^{k}_{i=1} \frac{y}{\hat{y}} y_i (\delta_{i,j} - y_j) W_2 \sigma(W_1 x ) (1- \sigma(W_1 x )) W_1 \\
				&= (\hat{y} - y) W_2 \sigma(W_1 x ) (1- \sigma(W_1 x )) W_1 \\
			\end{split}
		\end{equation*}	
	\end{soln}
	
	\clearpage
	
	\subsection*{Solution 5.2}	
	\begin{soln}
\begin{figure}[htbp]
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Learning curve by my model}
\includegraphics[width=50mm]{./img/my_learningcurve.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Test error by my model}
\includegraphics[width=50mm]{./img/my_testerror.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\end{figure}
	\end{soln}

	\subsection*{Solution 5.3}	
	\begin{soln}
\begin{figure}[htbp]
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Learning curve by pytorch model}
\includegraphics[width=50mm]{./img/pytorch_learningcurve.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Test error by pytorch model}
\includegraphics[width=50mm]{./img/pytorch_testerror.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\end{figure}

	\end{soln}
	
	\clearpage
	
	\subsection*{Solution 5.4.a}	
	\begin{soln}
\begin{figure}[htbp]
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Learning curve by pytorch model}
\includegraphics[width=50mm]{./img/my_lc_a.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Test error by pytorch model}
\includegraphics[width=50mm]{./img/my_er_a.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\end{figure}

	\end{soln}

	\subsection*{Solution 5.4.b}
	\begin{soln}
\begin{figure}[htbp]
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Learning curve by pytorch model}
\includegraphics[width=50mm]{./img/my_lc_b.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\begin{minipage}{0.5\hsize}
\begin{center}
\caption{Test error by pytorch model}
\includegraphics[width=50mm]{./img/my_er_b.png}
\end{center}
\captionsetup{labelformat=empty}
\end{minipage}
\end{figure}

	\end{soln}

		
\bibliographystyle{apalike}
	
		
	
	
	
	
	
	
	
	
	
	
\end{document}
